Example Code for the Attention Deployment Analysis Project

This code visualizes the visual attention of the Vision-Language Model LLaVA-OneVision while performing a Reference Expression Generation task with varying levels of noise.

To run the notebook, clone the project and update the file paths to correctly load the image and the vis_attn_matrix_average.pt file.

Since the code relies on the model's attention matrices, a minimum of 17GB of GPU memory is required to run it.
